# Fast R-CNN Object Detection with PennFudanDataset

This is a PyTorch implementation of object detection using Fast R-CNN on the PennFudanDataset.

## Dataset

The PennFudanDataset is a dataset for pedestrian detection and segmentation. It consists of 170 images with 345 instances of pedestrians, annotated with ground-truth bounding boxes and masks. The dataset can be downloaded from [here](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip).

The dataset has Pedestrain Images and their correspondin Masks.
The typical imgage is RBG (3, 559, 556): 

The following is an example:

![image](https://user-images.githubusercontent.com/40626584/222969497-7cf443bc-c407-47d8-97a7-b683e17a0000.png)

While the Masks looks like this with simple PIL imshow:

![image](https://user-images.githubusercontent.com/40626584/222969522-e94511ed-2f95-4f48-af98-c88fe47b0566.png)

This is so because segmentaion masks of pedestrians contains numbers (1, 2, 3..) depending on the number of pedestrians in the image.
You can see this by saving this numpy as csv and then applying conditional formatting using Excel.
The below figure shows the section of this. Here the Pedestrian 1 has all pixels filled with '1' which are highlighted in Red. while the background is white and filled with '0s'.

![image](https://user-images.githubusercontent.com/40626584/222969738-84ef846c-87fa-4c6e-b35d-f942bd3c0d6d.png)

Although on applying an appropriate pallette this mask looks like this:

![image](https://user-images.githubusercontent.com/40626584/222969626-3c3a3d81-3b93-46df-b8f3-49efc9255673.png)


## Requirements

- PyTorch 1.7+
- torchvision
- numpy
- matplotlib
- Pillow
- tqdm

## Training

To train the Fast R-CNN model on the PennFudanDataset clone the repository and run 
```
python model.py
```

This will train the model on the dataset for 10 epochs with a learning rate of 0.001 and a batch size of 4. You can modify the hyperparameters according to your needs.

## Concepts:
### What is an Anchorgenerator in Pytorch:
In PyTorch, an AnchorGenerator is a module used in object detection models to generate a set of anchor boxes at each spatial location on a feature map. Anchor boxes are used to represent object proposals and are typically defined as boxes with fixed aspect ratios and sizes at different locations in the image.

The AnchorGenerator module takes as input the size of the feature map, the aspect ratios and scales of the anchor boxes, and the stride of the feature map relative to the input image. It then generates a set of anchor boxes at each spatial location on the feature map, based on the specified aspect ratios, scales, and stride.

The AnchorGenerator module is often used in conjunction with the RPN (Region Proposal Network) module in object detection models such as Faster R-CNN and RetinaNet. The RPN module uses the anchor boxes generated by the AnchorGenerator module to propose regions of the image that are likely to contain objects, which are then passed to the object detection network for further processing.

Overall, the AnchorGenerator module plays a key role in the object detection pipeline, as it provides a set of anchor boxes that the model can use to identify objects in the input image.

For example:

![image](https://user-images.githubusercontent.com/40626584/222970109-aea8ac22-4c02-467b-a270-d143c6300cfa.png)


This visualization shows three different anchor boxes that are generated at a single location on a feature map using the AnchorGenerator module. The anchor boxes have different scales (0.5, 1.0, and 2.0) and a fixed aspect ratio of 0.5. Each anchor box is represented as a colored rectangle on the right-hand side of the diagram. These anchor boxes are used to propose regions of the input image that are likely to contain objects, which are then passed to the object detection network for further processing.

Here: Scale refers to the relative size of the box, while aspect ratio is the ratio of the width to the height of the anchor box.

### What is TorchVision MultAlign ROI
Torchvision Multiscale ROI (Region of Interest) Align is a PyTorch function used in object detection and instance segmentation tasks. It is an extension of the ROI Align function, which was introduced in Faster R-CNN for object detection. The ROI Align function takes an input feature map and a set of bounding box proposals and extracts a fixed-size feature map from each of the proposed regions.

The Multiscale ROI Align function builds on this by allowing the extraction of features at multiple scales for each proposed region. It works by first resizing the input feature map to several different scales and then performing the ROI Align operation on each scale. The resulting features are then combined into a single feature map using a learnable fusion operation.

Multiscale ROI Align can help improve the accuracy of object detection and instance segmentation models by allowing them to better handle objects of varying scales. It is often used in conjunction with other techniques, such as feature pyramid networks, to build more powerful and accurate models.

![image](https://user-images.githubusercontent.com/40626584/222969965-373d2c1c-0a55-4bbb-a6ef-7a59ada9fca9.png)

This blog provides a nice summary about this [here](https://kaushikpatnaik.github.io/annotated/papers/2020/07/04/ROI-Pool-and-Align-Pytorch-Implementation.html)




 
